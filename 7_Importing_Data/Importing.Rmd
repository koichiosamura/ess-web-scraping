---
title: "Importing Data"
author: "Marius Saeltzer"
date: "23 7 2023"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

Readings: Weidmann 2023: p. 39-58

a) Files 

   Binary versus Text Files

   Encodings



deeper into objects


attributes
names 



b) Tabular Data
    
    csv
    stata
    spss 
    
  
  Deep dive: managing complex datasets 
  
    searching
    NA checks
    Codebooks
    

c) Nested Data 

    json
    xml
    html
    
    
    
d)  Interpreting an API Call    

    The Power of Lists 

    Understanding what you do: a For-Loop 
  
    Functional Mapping 

e) Task: Parse your own Data

  Use the data you collected in the first half of the course and turn into datasets 
  
  
  
  #############
  
  

```{r}
if(!require(tidyverse)){install.packages("tidyverse")}
if(!require(readstata13)){install.packages("readstata13")}
if(!require(knitr)){install.packages("knitr")}
if(!require(jsonlite)){install.packages("jsonlite")}
if(!require(xml2)){install.packages("xml2")}
if(!require(foreign)){install.packages("foreign")}
if(!require(rvest)){install.packages("rvest")}

install.packages("RMySQL")
library("RMySQL")

library(jsonlite)
library(httr2)
library(tidyverse)
```


We talked about general forms of data files. How we can store information in a particular structure, and the consequences it has. We rarely are free to define how data looks like. We have to deal with what we have. Today, we talk about how to work with data in practice.


# Exercise 3:

Please note your answers to these questions (5 Minutes)

  What defines tabular/rectangular data?
  
  What are advantages of treelike data in comparison?
  
  What are possible problems when turning trees into tables?
  
  Indicate the problems you had yesterday when putting data into form.
  

First, we will learn how to implement transformations of data, moving it across levels of analysis and shapes. 

Second, we will learn how to import data into R. I know you already know how to do this, mostly, but we will go deep on what to do with reelike data, file systems, file encodings, remote databases etc. We will also learn tools to MAKE sense of new datasets we encounter in the wild.



# Session II: Importing Data into R

Now we see that R can be used to create and manipulate objects which contain values. In the end, all data operations can be reduced to this. But to do statstics, we will want to read in real data.


Data can be read in from two prototypical sources:


Files 

  textfiles
  
  binary files 
  

Databases


## FILES

The typical place we get data from as social scientists are files. Whether it is excel, csv or spss, they are typically transported as self-contained blobs of data. 

There are two broad classes:

  Text files: which only contain characters and are interpretable by the human eye
      
      csv
      json
      xml/html
        
  binary files: data that is not readibly without an intepretation help 
      
      image
      video
      executables
      compressed (zip,tar,...)
      Program files (rdata,rds)      
      Propietory Formats (Stata, SPSS etc.)
      
In general, you will find a tradeoff between compression (textfiles are larger) and reusability.
While textfiles will always be interpretable, binaries depend on programs to read them. If those are closed
source, you will be in trouble.




### Directories and the File System 

Both data sources can either be stored remotely or locally. Most of you will have experience accessing files locally, but maybe there are some useful tricks you can still learn.

The working directory is the place on a drive where R looks for data first. It allows you to directly call files without their exact location. Depending on the setting, it has to be defined (like in acient times) or you will have it defined by a) your R-Studio Project file or b) your document.

```{r}

#setwd('C:\Users\admin\Documents\Lehre\Lehre FSS 19\Data')

``` 

However. I suggest you do not rely on this. Setting working directories is considered bad practice, as this is something people always have to change when moving a project around. It is highly suggested you use projects or Mardown files. 

If you go old school and use a non-markdown, non-project environment, you can use this line of code:

```{r}
#setwd(dirname(rstudioapi::getActiveDocumentContext()$path))
```

It will set your working directory to where the document was placed. Avoid this if possible. 



## File Management



Instead, you should use relative pathes.

As we talk a lot about trees around here: your file system is exactly that. You can of course treat it as such an make use of it. You can navigate

./ go to a subdirectory from HERE  

../ UP from here 


R allows to manipulate directories!

Right now, we are in a directory where no local data is stored.
```{r}
list.files()

```
But we can go elsewhere of course!
```{r}
list.files("..")

```
gives you all the files of the Course. We can check what is in the data file from here.


```{r}

list.files("../data/")
```

And of course we change the FS also.

```{r}

dir.create("files")
dir.create("files/more_files")

list.files("./files")

```

### How to name files 


Machine-readable file names: 

Depending on the machine people are using, this can be problem or not, but you should always beware to 


NO SPACES!

NO SPECIAL CHARACTERS!

Please use ASCII. I fried my OS last week because university thought it would be great to call me users/Sältzer/ 


NO UPPER CASE! 

Linux (and accordingly, servers), are case senstive. 



File names that sort well 



### Building a directory

I like to build a research project like this:

```{r}
build<-F
if(build==T){
dir.create("tools") # for helper functions we use across scripts and load packages 
  
dir.create("data") # for data
  dir.create("data/raw") # as we get it
  dir.create("data/analysis") # as we process it for analysis

dir.create("plots") 
  dir.create("plots/main")
  dir.create("plots/appendix")

dir.create("results")
  dir.create("results/main")
  dir.create("results/appendix")

dir.create("scripts") # if you like to split your analysis into individual steps, this is useful 
# as you can then sequentially source them!
    
}
```

good example of a scripts folder:

01_data_collection.R
02_data_processing.R
03a_joining.R
03b_aggregation.R
04_analysis.R
05_plotting.R


### Files from the Internet

Read file remotely




# Session II: Importing Data into R

```{r}

#?read.table()

```

As you can see in the help file, the function has a number of arguments, most importantly FILE which tells the computer what to open. 
read.csv is a special case of read.table which has a lot decided before, like what kind of file it is supposed to open. The default is the
Comma-separated values file, the mighty CSV.

It is by far the most common way of storing data. This is what people mostly refer to as "excel-file", which is just a csv that is rendered unreadible for anyone unwilling to pay for it. Let's try it out. 






## Text Formats


Wie speichern wir nun Daten? 

#### csv


```{r}
c1<-read.csv('files/polls.csv')
```

Der Computer teilt Werte in Spalten durch das Komma auf. Wie Sie sehen können, interpretiert es standardmäßig die erste Zeile als Spaltennamen und importiert alle Zeichenfolgen als Faktoren.


Probieren wir mal die andere CSV aus. 



```{r}
c3<-read.csv("files/polls2.csv",row.names = NULL)
```

Das ist...chaos.

Was haben wir denn da gemacht? Wir haben eine "deutsche" csv erzeugt, mit Semikolon als Feldtrenner. Der Hauptgrund ist, dass wir in DE Kommata as DEZIMALTRENNER verwenden. Read.csv sucht als DEFAULT immer nach kommate. Er trennt die Daten also an den Dezimalstellen.


Tabstop
Komma
Semikolon
Doppelpunkt


Macht nichts...wir passen uns an.

```{r}
c3<-read.csv("files/polls2.csv",row.names = NULL,dec=",",sep = ";")
```


### The Terror of the ENCODINGS

One typical problem of many users is the encoding issue. But once we understood why we need encodings and how they work, we can deal with this.


Any language has to be expressed in a limited number of characters, which again have to be coded down to 1 and 0 for the computer to understand. Encodings are translation schemes. The more different characters you want to express, the more complex the 0-1 scheme has to become to create a higher number of unique characters. The simplest code, programming code, is coded in ASCII. A very compex coding scheme such as UTF-32, which can contain 2^32 different characters, will be 4x as memory intensive as UTF-8, which can only store 2^8 values. Accordingly, most institutions use 8-bit systems appropriate for their language and those very close to them. 


RStudio will automatically represent text following your OS configuration. For sensible systems (MacOS, Linux) this is always UTF-8. In window, this will depend on your own country version. 

```{r}

bt<-read.csv()

```

If not, we can use the rvest package!


```{r}
readr::guess_encoding(bt)
```

```{r}
iconv()
```



### R's very Own data 

R like Stata and SPSS has its own form of storing data.

You can save and load any object from R, even if it would not fit into a classical rectangular dataset. Of course, this can't be opened with other programs but can is very efficient in terms of speed and memory usage. Workspaces can contain dataframes or more complex objects like lists, regression models or even plots.

```{r}
load(url("https://github.com/msaeltzer/classified/blob/master/data/survey.rdata?raw=true"))


```

Every read function has a mirroring WRITE or save function

```{r}

save(gles,file='../data/survey.rdata')
writeRDS(gles,file='../data/survey.Rds')

```

RDS files are like workspaces, but are to be assigned. They do not keep the NAME and contain only ONE OBJECT.




```{r}
a2<-lapply(gles,attributes)
a3<-lapply(a2,function(x) x$label)
labels<-unlist(a3)
```







# Dealing with New Data 


!!!! Extracting variables

!!!! Plausability Checks

!!!! Missing Values 



Value Labels, Factors and all that shit

```{r}

```







### Tibbles 

Everything is some sort of data frame



### Data.table 

Everything is some sort of data frame




What can we do with a data set, once we have it?




## Dates

Date variables have very useful attributes. 

```{r}


class(d1$date)

d1$date<-as.Date(d1$date)

```

Here, this works fine, but what if you use other data formats?



```{r}

ger_birthdate<-c("01.01.2010","15.02.2015","01.07.1995","01.11.2003")

#df$ger_birthdate<-as.Date(df$ger_birthdate)

ger_birthdate<-as.Date(ger_birthdate,format = "%d.%m.%Y") # day, month, year 

ger_birthdate

```

POSIXct: sequence of integers
POSIXlt:list  

Sys.time()

```{r}
# print today's date
today <- Sys.Date()
format(today, format="%B %d %Y")
"June 20 2007"
```


%d	day as a number (0-31)	01-31
%a
%A	abbreviated weekday
unabbreviated weekday	Mon
Monday
%m	month (00-12)	00-12
%b
%B	abbreviated month
unabbreviated month	Jan
January
%y
%Y	2-digit year
4-digit year	07
2007




lubridate

posticx

Timediff

Origin

Timezones 

Exercises: Convert the following time stamps





## Complex Data Structures

Not all data can be stored in a rectangular form. Often data is closer to a list than a data frame. There are two main file formats that allow storing structured, not rectangular data. 

    XML
    
    JSON
    
Both types store information hierarchically and can therefore combine data sets that are related, but not combineable. For example, I will get data for the German Bundestag from abgeordnetenwatch.de    

### From JSON/XML

Using the jsonlite package, we have the importing function. In this case, we get data directly from the homepage!



```{r}
query<-"Marius Saeltzer"
resp <- request("https://api.semanticscholar.org/graph/v1/author/search") %>%
    req_url_query(query = query) %>%
    req_url_query(fields = "name,papers.title,papers.fieldsOfStudy",limit=1000L) %>%

    req_perform()
```


We can transform the JSON file into an R list, as the data has the same structure

```{r}
ll<-resp_body_json(resp)
```

As we can see, there are 2 Marius's!

```{r}
ll$data[[1]]$name

ll$data[[2]]$name


```
You can see the first merging problem that will haunt us tomorrow :)


So what can we do to unlist this stuff? Let's start like we have no idea 

```{r}


ll$data[[1]]$papers


```

We have 2 elements in this list, each has again 3 data points:
  
  PaperID
  Title
  FieldsOfStudy


Now we can see the dilemma of the JSON file: to unnest this data, we would have create a data structure das contains all the data, but in rectangular form. This could be a very wide dataset, like a variable for every paper, its id and its name. Or it could be a very long entry that contains every paper with the paper as unit of analysis. 

The deeper the data goes, if we add more information on the paper level, the deeper the unit of analysis will go. 
Let's do the unpacking you did in the API session briefly.


```{r}

find_scholar <- function(query) {
  resp <- request("https://api.semanticscholar.org/graph/v1/author/search") %>%
    req_url_query(query = query) %>%
    req_url_query(fields = "name,papers.title,papers.fieldsOfStudy",limit=1000L) %>%

    req_perform() %>%
    resp_body_json()
  data <- pluck(resp, "data")
  tibble(name = map_chr(data, "name"),
         author_Id = map_chr(data, "authorId"),
         papers =  map(data, "papers"))
}


d1<-find_scholar(query)

```




```{r}
# We build a function that allows to extract the authorship data

scholar_unnest<-function(x){
  if(nrow(x)>0){ # if data was retrieved for the query
  out <- x %>%
  unnest(papers) %>%
  unnest_wider(papers) %>%
  unnest(fieldsOfStudy)%>%
  unnest(fieldsOfStudy)}else(out<-x) # else keep the named list
return(out)}




d2<-scholar_unnest(d1)

```

There are just very few publications in this tiny file. Now imagine somebody has dozens of papers. Or we push the unit of analysis further down. 



## Data Project


As mentioned before, we want to do a little course project: collecting data on conferences. We start with the 2023 ECPR conference. You already scraped these lists. And now we will put them together. We use this list as a seed for furher data collection, make sense of who is who, and maybe even extract coauthoriship networks that allows combining them!

Let's import a seed dataset. 
```{r}
r1<-readRDS("seeds/ecpr_2023_data.rds")
```


Let's scale things up here for our database



```{r}
# documentation: https://api.semanticscholar.org/api-docs/graph#tag/Author-Data/operation/get_graph_get_author

# We use a seed list of authors from 2023 ECPR

aut<-unique(r1$authors)



## We will now turn the API-call into a loop, since we want to get data from all the users
## We use a for-loop. In general, don't do this in R, since it is slow. Here, as we call an API, it doesn't really matter
## and we get a maximum of control over the situation.


# We define an empty list to store the results in
scholars<-list()
for(i in 1:length(aut)){ # and loop over all authors
  print(i)
  scholars[[i]]<-find_scholar(aut[i]) # storing the data
  names(scholars)[i]<-aut[i] # and naming the list element after the search query
}

# Create data dir if not yet there
# Saving the data as RDS
saveRDS(scholars,file="../data/scholars.rds")
```


We now have a fast amout of hits...the question which of those is actually...a political scientist.


```{r}

likely_pol<-function(out){
  if(nrow(out)>0){
  out$fieldsOfStudy<-ifelse(is.na(out$fieldsOfStudy),"Unknown",out$fieldsOfStudy)
  out$pol<-out$fieldsOfStudy=="Political Science"
  pol1<-aggregate(pol~author_Id+name,data=out,FUN="mean")
  out$soc<-out$fieldsOfStudy=="Sociology"
  
  pol2<-aggregate(soc~author_Id+name,data=out,FUN="mean")
  out$comp<-out$fieldsOfStudy=="Computer Science"
  
  pol3<-aggregate(comp~author_Id+name,data=out,FUN="mean")
  pol4<-aggregate(comp~author_Id+name,data=out,FUN="length")
 
  pol1$sociology<-pol2$soc
  pol1$comp<-pol3$comp
  pol1$n<-pol4$comp
  names(pol1)
  }else{pol1<-data.frame(author_Id=NA,name=NA,pol=NA,n=NA)}
  return(pol1)

}


# Apply Loop: While we don't care about speed above, here we do: lapply maps a function and returns a list

sc2<-lapply(scholars,scholar_unnest)

for(i in 1:length(sc2)){

  likely_pol(sc2[[i]])

}



pol<-lapply(sc2,likely_pol)

```


``` {r}
x<-pol2[["Mariana Borges Martins da Silva"]]

remove_bad<-function(x){
  x<-x[x$pol|x$sociology>0,]
  x<-x[order(x$n),]
  return(x)

}

library(stringdist)

x$

# we have some systematic problems with spanish names and need to eliminate names that

pol2<-lapply(pol,remove_bad)


for(i in 1:length(pol2)){
  if(nrow(pol2[[i]])>0){
  pol2[[i]]$query<-names(pol2[i])
  pol2[[i]]$sdist<-stringdist(unique(pol2[[i]]$query),pol2[[i]]$name)
  }
}

pol3<-do.call(rbind,pol2)

```


We finally want to get the papers now!

```{r}

get_papers <- function(author_id) {
  request("https://api.semanticscholar.org/graph/v1/author/") %>%
    req_url_path_append(author_id) %>%
    req_url_path_append("papers") %>%
    req_url_query(fields = "externalIds,url,citationCount,citations,references",limit=1000L) %>%
    req_perform() %>%
    resp_body_json()
}

get_papers_l <- function(author_id) {
  request("https://api.semanticscholar.org/graph/v1/author/") %>%
    req_url_path_append(author_id) %>%
    req_url_path_append("papers") %>%
    req_url_query(fields = "externalIds,url,citationCount,authors",limit=1000L) %>%
    req_perform() %>%
    resp_body_json()
}

pol3$
y <- get_papers_l(77195066)

yy1<-y$data
length(yy1)
yy1[[1]]$authors

```


Unnest the data:

```{r}


papers<-list()

papers[[1765]]<-NA  # hierfür einen trycatch

for(i in 1766:nrow(pol3)){
  print(i)
  if(!is.na(pol3$author_Id[i])){
    papers[[i]]<-get_papers_l(pol3$author_Id[i])}else{infos[[i]]<-NA}
}


i<-1
j<-1
for(i in 1:length(papers)){

  p1<-papers[[i]]$data

  for(j in 1:length(p1)){

    p2<-p1[[j]]$authors
    for(k in 1:length(p2)){

      p2[[k]][[1]]
    }

  }

}
get_auth_id<-function(p2){lapply(p2,function(x) x$authors)}
```



```{r}
papers2<-list()

papers2[[1]]<-papers

p2<-xml2::as_xml_document(papers2)


```




```{r}


papers<-readRDS(papers,file="../../data/papers.rds")

```



Getting all coauthors


author - - - paper 1
                - paper ID
                - citation count
                - authors
                    - author 1
                    - author 2
                    - author 3
                        - author ID
                        - author name
           - paper 2

           - paper 3

Adress of author ID: author - paper - authors - ID
loop form:

```{r}




p1<-papers[[1]]$data


i<-1

coauthors<-c()



for(i in 1:length(p1)){ # for each paper

  authors<-p1[[i]]$authors
  auth<-pol3$author_Id[i] # adding original searched user ID
  for(j in 1:length(authors)){

    coauthors<-c(coauthors,authors[[j]]$authorId)

  }

  ca_data<-cbind.data.frame(author=auth,coauthors=coauthors)

}


```


```{r}

lapply(p1,)


get_author_data<-function(p1)

p2<-lapply(p1, function(x) x$authors)

lapply(p2,)




p2<-p2[[1]]

get_auth_id<-function(p2){unlist(lapply(p2,function(x) x$authorId))} # get all ids out of a paper entry

p2

x<-lapply(p2,get_auth_id)

x



for(i in 1:length(p1))
p1[[1]]$authors


p1[[3]]$authors

p11[[2]]$authorId
p11[[1]]$authorId

get_auth_id(p11)






get_paper_auth<-function(p1){lapply(p1$data,get_auth_id)$authors}

get_paper_auth(papers[[1]])


```





## Databases 






```{r}
library(RMySQL)
db_user <- 'Johannes'
db_password <- 'seNNahoj53'
db_name <- 'essex'
db_host <- '16.171.198.48' # for local access
db_port <- 3306

# 3. Read data from db
mydb <-  dbConnect(RMySQL::MySQL(), user = db_user, password = db_password,
                 dbname = db_name, host = db_host, port = db_port)





```


```{r}

dbListTables(mydb)

```



We now learned about how to represent data in a single object. Next week, we learn how to 

### Very VERY Basic SQL

FROM == [] or $ # describes an object
AS   == aliasing (change the names inside a query to make it shorter i.e)

SELECT == r[,i] # selects columns
WHERE == r[i,]  # Selects rows based on logic


DISTINCT == unique() # reduces to unique values
ORDER BY == order() # sorts data
LIKE == grepl()  # search by regular expression 
GROUP BY 

```{r}
s <- paste0("select * from ", db_table)
rs <- dbSendQuery(mydb, s)


```



## Accessing data

Now, we can use SQL to query our data 

Structure of a request: 
    COMMAND *
    FROM the_table_you_want; 
    
```{r}


personal$First.Name

dbGetQuery(us, ' 
           SELECT "First.Name" 
           FROM personal;                 
                              ')              

```



```{r}
dbGetQuery(us, ' 
           SELECT "First.Name","Name" 
           FROM candidates;                 
                              ')              
```


```{r}

personal[personal$Party=="Rep",1:2]

dbGetQuery(us, ' 
           SELECT "First.Name","Name" 
           FROM candidates                 
           WHERE "Party"=="Rep";                 
                              ')              
```

As you can see, this is quite intuitive and can be used to custom tailor your access just as in R itself.

Of course, you can just extract the whole table into a dataframe again using 
```{r}

df2<-dbGetQuery(us, ' 
           SELECT *
           FROM personal;')                

```
and do the data operations in R.

Now, after we practiced a bit, we can add our data in the same data base
```{r}

us <- dbConnect(RSQLite::SQLite(), "election_main.sqlite")

dbListTables(us)

## define the core database 
dbWriteTable(us, "twitteraccounts",accounts,overwrite=T)
dbWriteTable(us, "campaignfinance",candidates,overwrite=T)
dbListTables(us)



#Say bye bye and close the connection


```
